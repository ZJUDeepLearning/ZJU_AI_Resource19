{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dataProcess(df):\n",
    "    x_list, y_list = [], []\n",
    "    # df替换指定元素，将空数据填充为0\n",
    "    df = df.replace(['NR'], [0.0])\n",
    "    # astype() 转换array中元素数据类型\n",
    "    array = np.array(df).astype(float)\n",
    "    # 将数据集拆分为多个数据帧\n",
    "    for i in range(0, 4320, 18):\n",
    "        for j in range(24-9):\n",
    "            mat = array[i:i+18, j:j+9]\n",
    "            label = array[i+9, j+9] # 第10行是PM2.5\n",
    "            x_list.append(mat)\n",
    "            y_list.append(label)\n",
    "    x = np.array(x_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    return x, y, array\n",
    "\n",
    "def validate(x_val, y_val, weights, bias):\n",
    "    loss = 0\n",
    "    for i in range(400):\n",
    "        loss += abs(y_val[i] - weights.dot(x_val[i].reshape(-1,1)) - bias)\n",
    "    return loss / 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(x_train, y_train, x_val,y_val,epoch):\n",
    "    bias = 0 # 偏置值初始化\n",
    "    weights = np.ones(9) # 权重初始化\n",
    "    learning_rate = 1 # 初始学习率\n",
    "    reg_rate = 0.001 # 正则项系数\n",
    "    bg2_sum = 0 # 用于存放偏置值的梯度平方和\n",
    "    wg2_sum = np.zeros(9) # 用于存放权重的梯度平方和\n",
    "    x_val = x_val[:,9,:]\n",
    "    for i in range(epoch):\n",
    "        b_g = 0\n",
    "        w_g = np.zeros(9)\n",
    "        # 在所有数据上计算Loss_label的梯度\n",
    "        for j in range(3200):\n",
    "            y_pred = weights.dot(x_train[j, 9, :]) - bias\n",
    "            b_g += (y_train[j] - y_pred) * (-1)\n",
    "            for k in range(9):\n",
    "                w_g[k] += (y_train[j] - y_pred) * (-x_train[j, 9, k]) \n",
    "        # 求平均    \n",
    "        b_g /= 3200\n",
    "        w_g /= 3200\n",
    "        #  加上Loss_regularization在w上的梯度\n",
    "        for m in range(9):\n",
    "            w_g[m] += reg_rate * weights[m]\n",
    "        \n",
    "        # adagrad\n",
    "        bg2_sum += b_g**2\n",
    "        wg2_sum += w_g**2\n",
    "        # 更新权重和偏置\n",
    "        bias -= learning_rate/bg2_sum**0.5 * b_g\n",
    "        weights -= learning_rate/wg2_sum**0.5 * w_g\n",
    "        if i%10==0:\n",
    "            loss = validate(x_val, y_val, weights, bias)\n",
    "            print('The loss on val data in epoch %s is:%d'%(str(i),loss))\n",
    "    return weights, bias\n",
    "def main1():\n",
    "    df = pd.read_csv('train.csv', usecols=range(3,27))\n",
    "    x, y, _ = dataProcess(df)\n",
    "    x_train, y_train = x[0:3200], y[0:3200]\n",
    "    x_val, y_val = x[3200:3600], y[3200:3600]\n",
    "    epoch = 200 \n",
    "\n",
    "    w, b = train1(x_train, y_train, x_val,y_val,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on val data in epoch 0 is:21\n",
      "The loss on val data in epoch 10 is:8\n",
      "The loss on val data in epoch 20 is:7\n",
      "The loss on val data in epoch 30 is:7\n",
      "The loss on val data in epoch 40 is:6\n",
      "The loss on val data in epoch 50 is:6\n",
      "The loss on val data in epoch 60 is:6\n",
      "The loss on val data in epoch 70 is:6\n",
      "The loss on val data in epoch 80 is:6\n",
      "The loss on val data in epoch 90 is:6\n",
      "The loss on val data in epoch 100 is:6\n",
      "The loss on val data in epoch 110 is:6\n",
      "The loss on val data in epoch 120 is:5\n",
      "The loss on val data in epoch 130 is:5\n",
      "The loss on val data in epoch 140 is:5\n",
      "The loss on val data in epoch 150 is:5\n",
      "The loss on val data in epoch 160 is:5\n",
      "The loss on val data in epoch 170 is:5\n",
      "The loss on val data in epoch 180 is:5\n",
      "The loss on val data in epoch 190 is:5\n"
     ]
    }
   ],
   "source": [
    "main1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(x_train, y_train, x_val,y_val,epoch):\n",
    "    bias = 0 # 偏置值初始化\n",
    "    weights = np.ones((1,162)) # 权重初始化\n",
    "    learning_rate = 1 # 初始学习率\n",
    "    reg_rate = 0.001 # 正则项系数\n",
    "    bg2_sum = 0 # 用于存放偏置值的梯度平方和\n",
    "    wg2_sum = np.zeros((1,162)) # 用于存放权重的梯度平方和\n",
    "\n",
    "    for i in range(epoch):\n",
    "        b_g = 0\n",
    "        w_g = np.zeros((1,162))\n",
    "        # 在所有数据上计算Loss_label的梯度\n",
    "        for j in range(3200):\n",
    "            data = x_train[j].reshape(-1,1)\n",
    "            y_pred = weights.dot(data) - bias\n",
    "            b_g += (y_train[j] - y_pred) * (-1)\n",
    "            for k in range(162):\n",
    "                w_g[0,k] += (y_train[j] - y_pred) * (-data[k,0]) \n",
    "        # 求平均    \n",
    "        b_g /= 3200\n",
    "        w_g /= 3200\n",
    "        #  加上Loss_regularization在w上的梯度\n",
    "        for m in range(162):\n",
    "            w_g[0,m] += reg_rate * weights[0,m]\n",
    "        \n",
    "        # adagrad\n",
    "        bg2_sum += b_g**2\n",
    "        wg2_sum += w_g**2\n",
    "        # 更新权重和偏置\n",
    "        bias -= learning_rate/bg2_sum**0.5 * b_g\n",
    "        weights -= learning_rate/wg2_sum**0.5 * w_g\n",
    "        if i%10==0:\n",
    "            loss = validate(x_val, y_val, weights, bias)\n",
    "            print('The loss on val data in epoch %s is:%d'%(str(i),loss))\n",
    "    return weights, bias\n",
    "\n",
    "def main2():\n",
    "        df = pd.read_csv('train.csv', usecols=range(3,27))\n",
    "        x, y, _ = dataProcess(df)\n",
    "        x_train, y_train = x[0:3200], y[0:3200]\n",
    "        x_train\n",
    "        x_val, y_val = x[3200:3600], y[3200:3600]\n",
    "        epoch = 200 \n",
    "\n",
    "        w, b = train2(x_train, y_train, x_val,y_val,epoch) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on val data in epoch 0 is:21\n",
      "The loss on val data in epoch 10 is:9\n",
      "The loss on val data in epoch 20 is:8\n",
      "The loss on val data in epoch 30 is:8\n",
      "The loss on val data in epoch 40 is:8\n",
      "The loss on val data in epoch 50 is:8\n",
      "The loss on val data in epoch 60 is:8\n",
      "The loss on val data in epoch 70 is:8\n",
      "The loss on val data in epoch 80 is:8\n",
      "The loss on val data in epoch 90 is:8\n",
      "The loss on val data in epoch 100 is:8\n",
      "The loss on val data in epoch 110 is:8\n",
      "The loss on val data in epoch 120 is:8\n",
      "The loss on val data in epoch 130 is:8\n",
      "The loss on val data in epoch 140 is:7\n",
      "The loss on val data in epoch 150 is:7\n",
      "The loss on val data in epoch 160 is:7\n",
      "The loss on val data in epoch 170 is:7\n",
      "The loss on val data in epoch 180 is:7\n",
      "The loss on val data in epoch 190 is:7\n"
     ]
    }
   ],
   "source": [
    "main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca(x_train, y_train, x_val,y_val,k,epoch):\n",
    "    bias = 0 # 偏置值初始化\n",
    "    weights = np.ones((1,k)) # 权重初始化\n",
    "    learning_rate = 1 # 初始学习率\n",
    "    reg_rate = 0.001 # 正则项系数\n",
    "    bg2_sum = 0 # 用于存放偏置值的梯度平方和\n",
    "    wg2_sum = np.zeros((1,k)) # 用于存放权重的梯度平方和\n",
    "\n",
    "    for i in range(epoch):\n",
    "        b_g = 0\n",
    "        w_g = np.zeros((1,k))\n",
    "        # 在所有数据上计算Loss_label的梯度\n",
    "        for j in range(3200):\n",
    "            data = x_train[j].reshape(-1,1)\n",
    "            y_pred = weights.dot(data) - bias\n",
    "            b_g += (y_train[j] - y_pred) * (-1)\n",
    "            for x in range(k):\n",
    "                w_g[0,x] += (y_train[j] - y_pred) * (-data[x,0]) \n",
    "        # 求平均    \n",
    "        b_g /= 3200\n",
    "        w_g /= 3200\n",
    "        #  加上Loss_regularization在w上的梯度\n",
    "        for m in range(k):\n",
    "            w_g[0,m] += reg_rate * weights[0,m]\n",
    "        \n",
    "        # adagrad\n",
    "        bg2_sum += b_g**2\n",
    "        wg2_sum += w_g**2\n",
    "        # 更新权重和偏置\n",
    "        bias -= learning_rate/bg2_sum**0.5 * b_g\n",
    "        weights -= learning_rate/wg2_sum**0.5 * w_g\n",
    "        if i%10==0:\n",
    "            loss = validate(x_val, y_val, weights, bias)\n",
    "            print('The loss on val data in epoch %s is:%d'%(str(i),loss))\n",
    "    return weights, bias\n",
    "def main_pca(k,epoch):\n",
    "    df = pd.read_csv('train.csv', usecols=range(3,27))\n",
    "    x, y, _ = dataProcess(df)\n",
    "    x_train, y_train = x[0:3200], y[0:3200]\n",
    "    x_pca = x_train\n",
    "    x_pca = x_pca.reshape(3200,162)\n",
    "    \n",
    "    mean = np.sum(x_pca,0)\n",
    "    std = np.std(x_pca,0)\n",
    "    x_pca = (x_pca-mean)/std\n",
    "    \n",
    "    c = np.dot(x_pca.T,x_pca)/3200\n",
    "    s,u = np.linalg.eig(c)\n",
    "    z = np.dot(x.reshape(3600,162),u[:,:k])\n",
    "    x_train, y_train = z[0:3200], y[0:3200]\n",
    "    x_val, y_val = z[3200:3600], y[3200:3600]\n",
    "    \n",
    "    w, b = train_pca(x_train, y_train, x_val,y_val,k,epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on val data in epoch 0 is:1502\n",
      "The loss on val data in epoch 10 is:42\n",
      "The loss on val data in epoch 20 is:37\n",
      "The loss on val data in epoch 30 is:36\n",
      "The loss on val data in epoch 40 is:34\n",
      "The loss on val data in epoch 50 is:32\n",
      "The loss on val data in epoch 60 is:31\n",
      "The loss on val data in epoch 70 is:30\n",
      "The loss on val data in epoch 80 is:29\n",
      "The loss on val data in epoch 90 is:28\n",
      "The loss on val data in epoch 100 is:27\n",
      "The loss on val data in epoch 110 is:26\n",
      "The loss on val data in epoch 120 is:25\n",
      "The loss on val data in epoch 130 is:24\n",
      "The loss on val data in epoch 140 is:24\n",
      "The loss on val data in epoch 150 is:23\n",
      "The loss on val data in epoch 160 is:22\n",
      "The loss on val data in epoch 170 is:22\n",
      "The loss on val data in epoch 180 is:21\n",
      "The loss on val data in epoch 190 is:21\n",
      "The loss on val data in epoch 200 is:20\n",
      "The loss on val data in epoch 210 is:20\n",
      "The loss on val data in epoch 220 is:20\n",
      "The loss on val data in epoch 230 is:19\n",
      "The loss on val data in epoch 240 is:19\n",
      "The loss on val data in epoch 250 is:19\n",
      "The loss on val data in epoch 260 is:19\n",
      "The loss on val data in epoch 270 is:18\n",
      "The loss on val data in epoch 280 is:18\n",
      "The loss on val data in epoch 290 is:18\n",
      "The loss on val data in epoch 300 is:18\n",
      "The loss on val data in epoch 310 is:17\n",
      "The loss on val data in epoch 320 is:17\n",
      "The loss on val data in epoch 330 is:17\n",
      "The loss on val data in epoch 340 is:17\n",
      "The loss on val data in epoch 350 is:17\n",
      "The loss on val data in epoch 360 is:17\n",
      "The loss on val data in epoch 370 is:17\n",
      "The loss on val data in epoch 380 is:17\n",
      "The loss on val data in epoch 390 is:16\n",
      "The loss on val data in epoch 400 is:16\n",
      "The loss on val data in epoch 410 is:16\n",
      "The loss on val data in epoch 420 is:16\n",
      "The loss on val data in epoch 430 is:16\n",
      "The loss on val data in epoch 440 is:16\n",
      "The loss on val data in epoch 450 is:16\n",
      "The loss on val data in epoch 460 is:16\n",
      "The loss on val data in epoch 470 is:16\n",
      "The loss on val data in epoch 480 is:16\n",
      "The loss on val data in epoch 490 is:16\n",
      "The loss on val data in epoch 500 is:16\n",
      "The loss on val data in epoch 510 is:16\n",
      "The loss on val data in epoch 520 is:16\n",
      "The loss on val data in epoch 530 is:16\n",
      "The loss on val data in epoch 540 is:16\n",
      "The loss on val data in epoch 550 is:16\n",
      "The loss on val data in epoch 560 is:16\n",
      "The loss on val data in epoch 570 is:16\n",
      "The loss on val data in epoch 580 is:16\n",
      "The loss on val data in epoch 590 is:16\n",
      "The loss on val data in epoch 600 is:16\n",
      "The loss on val data in epoch 610 is:16\n",
      "The loss on val data in epoch 620 is:15\n",
      "The loss on val data in epoch 630 is:15\n",
      "The loss on val data in epoch 640 is:15\n",
      "The loss on val data in epoch 650 is:15\n",
      "The loss on val data in epoch 660 is:15\n",
      "The loss on val data in epoch 670 is:15\n",
      "The loss on val data in epoch 680 is:15\n",
      "The loss on val data in epoch 690 is:15\n",
      "The loss on val data in epoch 700 is:15\n",
      "The loss on val data in epoch 710 is:15\n",
      "The loss on val data in epoch 720 is:15\n",
      "The loss on val data in epoch 730 is:15\n",
      "The loss on val data in epoch 740 is:15\n",
      "The loss on val data in epoch 750 is:15\n",
      "The loss on val data in epoch 760 is:15\n",
      "The loss on val data in epoch 770 is:15\n",
      "The loss on val data in epoch 780 is:15\n",
      "The loss on val data in epoch 790 is:15\n",
      "The loss on val data in epoch 800 is:15\n",
      "The loss on val data in epoch 810 is:15\n",
      "The loss on val data in epoch 820 is:15\n",
      "The loss on val data in epoch 830 is:15\n",
      "The loss on val data in epoch 840 is:15\n",
      "The loss on val data in epoch 850 is:15\n",
      "The loss on val data in epoch 860 is:15\n",
      "The loss on val data in epoch 870 is:15\n",
      "The loss on val data in epoch 880 is:15\n",
      "The loss on val data in epoch 890 is:15\n",
      "The loss on val data in epoch 900 is:15\n",
      "The loss on val data in epoch 910 is:15\n",
      "The loss on val data in epoch 920 is:15\n",
      "The loss on val data in epoch 930 is:15\n",
      "The loss on val data in epoch 940 is:15\n",
      "The loss on val data in epoch 950 is:15\n",
      "The loss on val data in epoch 960 is:15\n",
      "The loss on val data in epoch 970 is:15\n",
      "The loss on val data in epoch 980 is:15\n",
      "The loss on val data in epoch 990 is:15\n"
     ]
    }
   ],
   "source": [
    "main_pca(20,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
